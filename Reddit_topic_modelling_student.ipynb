{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4d2cebf-5a56-4c52-be20-ee365d201382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# add the rest of your imports below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aba487-dc84-42a5-a17f-0888a1da1b00",
   "metadata": {},
   "source": [
    "---\n",
    "# Data loading and preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c56fd1-ac5d-47a3-804b-d21f4a0fb25d",
   "metadata": {},
   "source": [
    "1. Read your assigned dataset as a **Pandas** dataframe, call it `df`\n",
    "2. keep only the columns \"author_id\", \"title\", and \"selftext\" \n",
    "3. Use the package **Redditcleaner** to remove specific _markdown_ placed by Reddit (use the `apply` function in Pandas for this)\n",
    "4. Standardise apostrophes to be all the straight and not curly \"â€™\" to \"'\" (a simple string replace should do it)\n",
    "5. Use the subpackage **Contractions** from the **contractions** package to expand English contractions (I'm, we'll, etc.)\n",
    "6. Investigate how to use the regular expressions package **re** to remove numbers, special characters and punctuation (keeping only alphabetic letters)\n",
    "7. Starndardise all titles and selftext to be lower-case\n",
    "8. Create a new column `fulltext` that is `title` + white_space + `selftext`\n",
    "9. Create a new column `tokenised` that contains the `fulltext` string as a list of words\n",
    "10. Use the stopwords (English) in the package **NLTK** to remove stopwords from the `tokenised` column\n",
    "11. Use the lematisation function in the **Spacy** package to replace each work in the `tokenised` lists by its lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb5124cd-88c9-4542-bc29-8aa751c0ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your functions here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18082d0-067c-4f05-9370-6b790d1ad151",
   "metadata": {},
   "source": [
    "---\n",
    "# Corpus data as a matrix: TF-IDF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa1def-9c2a-4271-b7c7-ec8bcef9f2ae",
   "metadata": {},
   "source": [
    "Assume we have a corpus with $n$ documents, $D = \\{d_1, d_2, ..., d_n\\}$ Each $d_i$ is originally a string. However, after running the preprocessing steps above, $d_i$ is a list of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3c828-80e1-45b1-a8e9-06f6f7918cce",
   "metadata": {},
   "source": [
    "## Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd159154-02d4-4a1f-b487-c7c67d5677d9",
   "metadata": {},
   "source": [
    "The next step is to instantiate a variable that contains the corpus' vocabulary $V$, which is simply a set of all the tokens that appear at least once in at least one document.\n",
    "\n",
    "1. Instantiate the variable `vocabulary` with the corpus' vocabulary below. Use a single instruction (i.e. not a loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789a0b6-877c-4164-9c90-0685cdfd0bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7602dd7-d883-4f1e-b753-1de397c42e0d",
   "metadata": {},
   "source": [
    "## Normalised Term-Frequency\n",
    "\n",
    "2. The next step is to create a pandas dataframe or numpy (matrix $A$, call it `tf` in your code) were rows are the tokens in the vocabulary, and columns are the documents in the corpus. Each matrix element $a_{i,j}$ contains the number of times token $i$ appears in document $j$ divided by the total number of tokens in document $j$. This is known as **Term Frequency** or **TF**. \n",
    "\n",
    "Notice that TF is a quantitative measure that relates words in the vocabulary and individual documents.\n",
    "\n",
    "Write a function, `term_frequency(word, document)` that computes the normalised TF for the given parameters `word` and `document`, where `document` is tokenised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9bf05-ab68-425e-ab5f-0e26c8db81bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ad634-0986-416e-b21e-75d4dce32092",
   "metadata": {},
   "source": [
    "## Inverse Document frequency\n",
    "\n",
    "3. The next step is to create an array where each element corresponds to a word in the vocabulary, and the vector element contains the quantitative measure known as **Inverse Document Frequency** or **IDF** defined by\n",
    "\n",
    "$\n",
    "IDF(\\text{word, corpus}) = \\log{\\left(\\frac{n}{df(\\text{word}_i)}\\right)}\n",
    "$\n",
    "\n",
    "where $n$ is the number of documents in the corpus, and $df(\\text{word}_i)$ is the number of documents where $\\text{word}_i$ appears at least once. Do not count the number of times the word appears, only add 1 to df if the word appears, and 0 otherwise. Your IDF must be a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df0ea5-d4d0-4d28-ac40-2e9f530fffb0",
   "metadata": {},
   "source": [
    "## Pruning \n",
    "\n",
    "We want to use a subset of the vocabulary that gives us the most informative words. For Pruning our vocabulary we need to think about the meaning of the IDF measure.\n",
    "\n",
    "What does it mean when the values are very high? very low?\n",
    "\n",
    "Think about a way to write a function that keeps more informative words, in the sense of partitioning the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76141f3c-fbeb-4b70-8d22-20eab10b3bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f02699-a492-492a-a1a4-57ffb6915f5c",
   "metadata": {},
   "source": [
    "## TF-IDF matrix\n",
    "\n",
    "4. Instantiate the matrix `tf-idf` as the same matrix `tf` but where each row is divided by the IDF of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05bb462-1e6d-489f-9bd8-a7e0329a1198",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10534a92-9af8-4dc5-b77a-ffe57b0d57ff",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Now you have your corpus represented as a matrix, explore what you can learn by analysing the cosine_similarity measure between documents (columns)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
